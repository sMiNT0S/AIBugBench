name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  PYTHON_DEFAULT_VERSION: '3.12'  # Default version for lint/type-check job

jobs:
  lint-and-type-check:
    name: Lint and Type Check
    # Explicit least-privilege permissions (B1)
    permissions:
      contents: read
      actions: read
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ env.PYTHON_DEFAULT_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install ruff mypy pytest
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-
      - name: Lint with ruff
        run: ruff check .
      - name: Type check with mypy
        run: mypy .
      - name: Format check with ruff
        run: ruff format --check .

  test-coverage:
    name: Test Coverage Analysis
    needs: lint-and-type-check
    permissions:
      contents: read
      actions: read
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ env.PYTHON_DEFAULT_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov coverage
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-
      
      - name: Run tests with coverage
        run: |
          pytest tests/ -v \
            --cov=benchmark \
            --cov=run_benchmark \
            --cov-report=html \
            --cov-report=xml \
            --cov-report=term
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: always()
        with:
          files: ./coverage.xml
          fail_ci_if_error: false
      
      - name: Upload coverage HTML report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: coverage-report
          path: htmlcov/
          retention-days: 30
      
      - name: Coverage summary
        run: |
          echo "## Test Coverage Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          coverage report --format=markdown >> $GITHUB_STEP_SUMMARY

  multi-platform-tests:
    name: Multi-Platform Tests
    needs: test-coverage
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.10', '3.11', '3.12', '3.13']
        include:
          - os: ubuntu-latest
            platform_name: linux
          - os: windows-latest
            platform_name: windows
          - os: macos-latest
            platform_name: macos
    runs-on: ${{ matrix.os }}
    permissions:
      contents: read
      actions: read
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov coverage
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-
      
      - name: Run comprehensive test suite
        run: |
          echo "Running comprehensive test suite on ${{ matrix.os }} with Python ${{ matrix.python-version }}"
          pytest tests/ -v --tb=short -m "not slow"
        continue-on-error: true
      
      - name: Run unit tests (fallback)
        if: failure()
        run: pytest tests/test_imports.py -v --tb=short
        continue-on-error: true
      
      - name: Setup test environment
        run: python setup.py
      
      - name: Validate test data structure
        shell: bash
        run: |
          echo "Validating test data files exist..."
          test -f test_data/process_records.py
          test -f test_data/user_data.json
          test -f test_data/config.yaml
          echo "✅ Test data validation passed"
      
      - name: Run platform-specific benchmark validation
        run: python benchmark/platform_validator.py
        continue-on-error: true
      
      - name: Collect benchmark artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-${{ matrix.platform_name }}-py${{ matrix.python-version }}
          path: |
            ci_results/
            results/
            *.log
          retention-days: 30

  benchmark-consistency-check:
    name: Benchmark Consistency Validation
    needs: multi-platform-tests
    runs-on: ubuntu-latest
    if: always()
    permissions:
      contents: read
      actions: read
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ env.PYTHON_DEFAULT_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
      
      - name: Download all benchmark artifacts
        uses: actions/download-artifact@v5
        with:
          pattern: benchmark-results-*
          path: ./collected-results
          merge-multiple: true
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run cross-platform comparison
        id: comparison
        shell: bash
        run: python scripts/compare_benchmarks.py ./collected-results
        continue-on-error: false
      
      - name: Generate consistency report
        run: |
          echo "# Benchmark Consistency Report" > consistency_report.md
          echo "" >> consistency_report.md
          echo "**Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> consistency_report.md
          echo "**Commit:** ${{ github.sha }}" >> consistency_report.md
          echo "" >> consistency_report.md
          
          if ls ./collected-results/platform_validation_*.json 1> /dev/null 2>&1; then
            echo "## Platform Results" >> consistency_report.md
            for file in ./collected-results/platform_validation_*.json; do
              echo "### $(basename "$file")" >> consistency_report.md
              python -c "
              import json
              with open('$file', 'r') as f:
                  data = json.load(f)
              print(f'- **Platform:** {data.get(\"platform\", \"unknown\")}')
              print(f'- **Score:** {data.get(\"total_score\", 0):.2f}/100')
              print(f'- **Within Tolerance:** {data.get(\"within_tolerance\", False)}')
              if 'full_results' in data and 'execution_time' in data['full_results']:
                  print(f'- **Execution Time:** {data[\"full_results\"][\"execution_time\"]:.2f}s')
              " >> consistency_report.md
              echo "" >> consistency_report.md
            done
          else
            echo "⚠️ No platform validation results found" >> consistency_report.md
          fi
      
      - name: Upload consistency report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: consistency-report
          path: |
            consistency_report.md
            collected-results/
          retention-days: 90

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    permissions:
      contents: read
      actions: read
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ env.PYTHON_DEFAULT_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
      
      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-
      
      - name: Run bandit security scan
        # Scan full codebase excluding user submissions & template scaffolds (reference impls included)
        run: bandit -q -r . -x tests,docs,submissions/user_submissions,submissions/templates -f json -o bandit-report.json
        continue-on-error: true
      
      - name: Run safety check
        run: safety check --json --output safety-report.json
        continue-on-error: true
      
      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
          retention-days: 30