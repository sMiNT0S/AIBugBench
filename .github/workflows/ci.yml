name: CI

env:
  PYTHON_DEFAULT_VERSION: "3.13"
  BANDIT_VERSION: "1.8.6"
  RUFF_VERSION: "0.12.11" # Upgraded to align with local environment (supports UP047 rule)
  MYPY_VERSION: "1.11.2"
  PYTEST_VERSION: "8.3.2"
  PYTEST_COV_VERSION: "5.0.0"
  COVERAGE_VERSION: "7.6.1"
  SAFETY_VERSION: "3.2.5"

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# (Moved to top-level env block)

jobs:
  action-pin-check:
    name: Actions Pin Verification
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332
      - name: Verify all actions pinned to full SHAs
        run: |
          if grep -R "uses: .*@v[0-9]" .github/workflows/*.yml; then
            echo "Found unpinned actions. Pin to full SHAs." >&2
            exit 1
          fi
  lock-verification:
    name: Dependency Lock Sync (runtime + dev)
    needs: action-pin-check
    permissions:
      contents: read
      actions: read
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332
      - name: Set up Python ${{ env.PYTHON_DEFAULT_VERSION }}
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
      - name: Verify lock files are in sync (pinned pip-tools)
        run: |
          python -m pip install --upgrade pip 'pip-tools==7.5.0'
          python -c "import importlib.metadata as m; print('pip-tools', m.version('pip-tools'))"
          python scripts/verify_lock_sync.py

  lint-and-type-check:
    name: Lint and Type Check
    needs: lock-verification
    # Explicit least-privilege permissions (B1)
    permissions:
      contents: read
      actions: read
    runs-on: ubuntu-latest
    steps:
      - name: Base Python + dev tools
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@ab149f501046408f307d3ac6a7c0991e08b74d61
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
          install-dev: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809 # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-
      - name: Lint with ruff
        run: ruff check .
      # Single mypy pass with Python 3.13 (current standard)
      # Explicit targets (no src/ layout): benchmark/, validation/, and run_benchmark.py.
      - name: "mypy (Python 3.13)"
        run: |
          python -m mypy ./benchmark ./validation ./run_benchmark.py --no-error-summary
      - name: Format check with ruff
        run: ruff format --check .
      - name: CLI smoke test
        run: python -c "import run_benchmark as m; assert callable(getattr(m,'main',None))"

  test-coverage:
    name: Test Coverage Analysis
    needs: lint-and-type-check
    permissions:
      contents: read
      actions: read
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    runs-on: ubuntu-latest
    steps:
      - name: Base Python + test tooling
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@ab149f501046408f307d3ac6a7c0991e08b74d61
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
          install-test: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809 # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-

      - name: Run tests (branch coverage unified)
        run: |
          coverage erase
          pytest -q \
            --cov=benchmark \
            --cov=validation \
            --cov=run_benchmark \
            --cov-branch \
            --cov-report=term-missing \
            --cov-report=xml:coverage.xml \
            --cov-report=html:htmlcov
          coverage combine || true
          coverage report -m

      - name: Coverage summary
        run: |
          echo "## Test Coverage (Branch Mode)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          coverage report --format=markdown >> $GITHUB_STEP_SUMMARY

      - name: Detect coverage.xml presence
        id: coverage_file
        shell: bash
        run: |
          if [ -f coverage.xml ]; then
            echo "present=true" >> "$GITHUB_OUTPUT"
            echo "coverage.xml present"
          else
            echo "present=false" >> "$GITHUB_OUTPUT"
            echo "coverage.xml missing"
          fi

      # New: publish the HTML coverage report for PR browsing
      - name: Upload HTML coverage
        uses: actions/upload-artifact@65462800fd760344b1a7b4382951275a0abb4808
        with:
          name: htmlcov
          path: htmlcov
          if-no-files-found: warn
          retention-days: 7
      # Detect presence of Codecov secret without job-level env
      - name: Detect Codecov token
        shell: bash
        run: |
          if [ -n "${{ secrets.CODECOV_TOKEN }}" ]; then
          echo "HAVE_CODECOV=1" >> "$GITHUB_ENV"
          else
          echo "HAVE_CODECOV=0" >> "$GITHUB_ENV"
          fi

      - name: Upload to Codecov
        if: ${{ env.HAVE_CODECOV == '1' && hashFiles('coverage.xml') != '' }}
        uses: codecov/codecov-action@fdcc8476540edceab3de004e990f80d881c6cc00
        with:
          files: coverage.xml
          token: ${{ secrets.CODECOV_TOKEN }}
          fail_ci_if_error: true

      - name: Skip Codecov (no token present)
        if: ${{ env.HAVE_CODECOV != '1' || steps.coverage_file.outputs.present != 'true' }}
        run: |
          if [ "$HAVE_CODECOV" != '1' ]; then
            echo "Codecov token not provided; skipping upload (private repo)."
          elif [ "${{ steps.coverage_file.outputs.present }}" != 'true' ]; then
            echo "coverage.xml missing; skipping Codecov upload."
          fi
  multi-platform-tests:
    name: Multi-Platform Tests
    needs: test-coverage
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.13"]
        include:
          - os: ubuntu-latest
            platform_name: linux
          - os: windows-latest
            platform_name: windows
          - os: macos-latest
            platform_name: macos
    runs-on: ${{ matrix.os }}
    permissions:
      contents: read
      actions: read
    steps:
      - name: Base Python + test tooling
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@ab149f501046408f307d3ac6a7c0991e08b74d61
        with:
          python-version: ${{ matrix.python-version }}
          install-test: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809 # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-
      - name: Install package (editable)
        run: python -m pip install -e .

      - name: Run comprehensive test suite
        run: |
          echo "Running comprehensive test suite on ${{ matrix.os }} with Python ${{ matrix.python-version }}"
          pytest tests/ -v --tb=short -m "not slow"

      - name: Run unit tests (fallback)
        if: failure()
        run: |
          echo "Full suite failed; executing minimal import smoke test for diagnostics..."
          pytest tests/test_imports.py -v --tb=short || true
          echo "Primary test suite failure retained (job will fail)."

      - name: Validate test data structure
        shell: bash
        run: |
          echo "Validating test data files exist..."
          test -f test_data/process_records.py
          test -f test_data/user_data.json
          test -f test_data/config.yaml
          echo "Test data validation passed"

      - name: Run platform-specific benchmark validation
        run: python benchmark/platform_validator.py
        continue-on-error: true

      - name: Collect benchmark artifacts
        uses: actions/upload-artifact@65462800fd760344b1a7b4382951275a0abb4808
        if: always()
        with:
          name: benchmark-results-${{ matrix.platform_name }}-py${{ matrix.python-version }}
          path: |
            ci_results/
            results/
            *.log
          retention-days: 30

  benchmark-consistency-check:
    name: Benchmark Consistency Validation
    needs: multi-platform-tests
    runs-on: ubuntu-latest
    if: always()
    permissions:
      contents: read
      actions: read
    steps:
      - uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332
      - name: Base Python setup
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@ab149f501046408f307d3ac6a7c0991e08b74d61
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Download all benchmark artifacts
        uses: actions/download-artifact@65a9edc5881444af0b9093a5e628f2fe47ea3b2e
        with:
          pattern: benchmark-results-*
          path: ./collected-results
          merge-multiple: true

      - name: Ensure collected-results exists
        shell: bash
        run: |
          if [ ! -d ./collected-results ] || \
             [ -z "$(ls -A ./collected-results 2>/dev/null)" ]; then
            echo "No collected benchmark artifacts found. Ensure previous jobs uploaded artifacts." >&2
            echo "Expected artifacts from multi-platform-tests (pattern: benchmark-results-*)" >&2
            exit 1
          fi

      - name: Run cross-platform comparison
        id: comparison
        shell: bash
        run: python scripts/compare_benchmarks.py --dir ./collected-results
        continue-on-error: false

      - name: Generate consistency report
        run: |
          echo "# Benchmark Consistency Report" > consistency_report.md
          echo "" >> consistency_report.md
          echo "**Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> consistency_report.md
          echo "**Commit:** ${{ github.sha }}" >> consistency_report.md
          echo "" >> consistency_report.md

          if ls ./collected-results/platform_validation_*.json 1> /dev/null 2>&1; then
            echo "## Platform Results" >> consistency_report.md
            for file in ./collected-results/platform_validation_*.json; do
              echo "### $(basename "$file")" >> consistency_report.md
              python -c "
              import json
              with open('$file', 'r') as f:
                  data = json.load(f)
              print(f'- **Platform:** {data.get(\"platform\", \"unknown\")}')
              print(f'- **Score:** {data.get(\"total_score\", 0):.2f}/100')
              print(f'- **Within Tolerance:** {data.get(\"within_tolerance\", false)}')
              if 'full_results' in data and 'execution_time' in data['full_results']:
                  print(f'- **Execution Time:** {data[\"full_results\"][\"execution_time\"]:.2f}s')
              " >> consistency_report.md
              echo "" >> consistency_report.md
            done
          else
            echo "No platform validation results found" >> consistency_report.md
          fi

      - name: Upload consistency report
        uses: actions/upload-artifact@65462800fd760344b1a7b4382951275a0abb4808
        if: always()
        with:
          name: consistency-report
          path: |
            consistency_report.md
            collected-results/
          retention-days: 90

  security-scan:
    name: Security Scan
    needs: lock-verification
    runs-on: ubuntu-latest
    permissions:
      contents: read
      actions: read
    steps:
      - name: Base Python + security tooling
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@ab149f501046408f307d3ac6a7c0991e08b74d61
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
          install-security: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809 # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.PYTHON_DEFAULT_VERSION }}-

      - name: Run bandit security scan
        # Use pyproject.toml configuration for consistent exclusions (pinned version)
        run: |
          bandit \
            -r . \
            -ll \
            --configfile pyproject.toml \
            -f json \
            -o bandit-report.json \
            --version
        continue-on-error: true

      - name: Run safety scan
        run: |
          safety scan \
            --file=requirements.lock \
            --json \
            > safety-report.json \
            || echo "safety scan fallback"
        continue-on-error: true
      - name: Run internal security audit
        run: python scripts/security_audit.py --json --output audit.json
        continue-on-error: true

      - name: Fail if audit not ok
        run: |
          # Inline Python audit verification wrapped for readability (was a single long line)
          python - <<'PY'
          import json, sys, pathlib
          data = json.loads(pathlib.Path('audit.json').read_text())
          ok = bool(data.get('ok'))
          print('Audit OK:', ok)
          sys.exit(0 if ok else 1)
          PY
        continue-on-error: false

      - name: Generate security audit summary
        if: always()
        run: python scripts/generate_audit_summary.py audit.json || true

      - name: Upload security reports
        uses: actions/upload-artifact@65462800fd760344b1a7b4382951275a0abb4808
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
            audit.json
            audit_summary.md
          retention-days: 30
