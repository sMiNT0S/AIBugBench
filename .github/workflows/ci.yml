name: CI

env:
  PYTHON_DEFAULT_VERSION: "3.13.7"
  BANDIT_VERSION: "1.8.6"
  RUFF_VERSION: "0.12.11"  # Upgraded to align with local environment (supports UP047 rule)
  MYPY_VERSION: "1.11.2"
  PYTEST_VERSION: "8.3.2"
  PYTEST_COV_VERSION: "5.0.0"
  COVERAGE_VERSION: "7.6.1"
  SAFETY_VERSION: "3.2.5"

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      validate_telemetry:
        description: "Enable telemetry schema validation"
        required: false
        default: false
        type: boolean

concurrency:
  group: ${{github.workflow}}-${{github.ref}}
  cancel-in-progress: true

# (Moved to top-level env block)

jobs:
  action-pin-check:
    name: Actions Pin Verification
    runs-on: ubuntu-24.04
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
      - name: Verify all actions pinned to full SHAs
        run: |
          set -euo pipefail
          echo "Verifying that all actions are pinned to full SHAs..."
          offenders=$(grep -RnE '^[[:space:]]*uses:[[:space:]][^@]+@([^[:space:]]+)' .github/workflows/*.yml | \
            awk '
              {
                line=$0;
                if (match(line, /@([^[:space:]]+)/, m)) {
                  ref=m[1];
                  # Accept only exact 40-hex commit SHAs
                  if (ref ~ /^[0-9a-fA-F]{40}$/) {
                    next;
                  } else {
                    print line;
                  }
                }
              }
            ' || true)
          if [ -n "$offenders" ]; then
            echo "Found unpinned or non-SHA action refs:" >&2
            echo "$offenders" >&2
            echo "Pin all actions to full 40-character commit SHAs." >&2
            exit 1
          fi
          echo "All actions are pinned to full SHAs."

  lock-verification:
    name: Dependency Lock Sync (runtime + dev)
    needs: action-pin-check
    permissions:
      contents: read
      actions: read
      pull-requests: write
      checks: write
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
      - name: Set up Python ${{ env.PYTHON_DEFAULT_VERSION }}
        uses: actions/setup-python@e797f83bcb11b83ae66e0230d6156d7c80228e7c

        with:
          python-version: ${{env.PYTHON_DEFAULT_VERSION}}
          check-latest: false
      - name: Debug lock environment
        shell: bash
        run: |
          set -eu
          python --version
          python - <<'PY' || true
          try:
              import importlib.metadata as m
              print('pip-tools', m.version('pip-tools'))
          except Exception:
              print('pip-tools not installed yet')
          PY
          git config --global core.autocrlf false || true
      - name: Verify dependency lock determinism
        env:
          PYTHONUTF8: "1"
        run: |
          python -m pip install -U pip "pip-tools==7.5.0"
          # Uses our canonicalizer: LF-only write + sorted/deduped '# via ...'
          python scripts/update_requirements_lock.py --check || (echo "runtime lock drift"; exit 3)
          python scripts/update_requirements_lock.py --check --dev || (echo "dev lock drift"; exit 3)
      - name: Verify dependency lock determinism (JSON)
        if: always()
        shell: bash
        id: lock_check_json
        run: |
          python -m pip install "pip-tools==7.5.0"
          python scripts/update_requirements_lock.py --check --json || true
          python scripts/update_requirements_lock.py --check --dev --json || true

      # --- Append-only telemetry/schema validators (read-only; no runner/step renames) ---
      - name: Collect telemetry (NDJSON, read-only)
        if: always()
        shell: bash
        run: |
          mkdir -p .ci/local
          : > .ci/local/telemetry.ndjson
          python scripts/update_requirements_lock.py --check --json       >> .ci/local/telemetry.ndjson || true
          python scripts/update_requirements_lock.py --check --dev --json >> .ci/local/telemetry.ndjson || true

      # Enable validation only when a local flag file is present (keeps this local-only)
      - name: Detect local telemetry flag
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          INPUT_FLAG="${{ inputs.validate_telemetry }}"
          VAR_FLAG="${{ vars.TELEMETRY_VALIDATE }}"
          if [ "$INPUT_FLAG" = "true" ] || [ "$VAR_FLAG" = "1" ] || [ -f .ci/local/enable_telemetry_validate ]; then
            echo "telemetry: enabled (input=$INPUT_FLAG, var=$VAR_FLAG, file=$([ -f .ci/local/enable_telemetry_validate ] && echo 1 || echo 0))"
            echo "TELEMETRY_VALIDATE=1" >> "$GITHUB_ENV"
          else
            echo "telemetry: disabled (input=$INPUT_FLAG, var=$VAR_FLAG)"
          fi

      - name: Validate telemetry schema
        if: ${{ always() && env.TELEMETRY_VALIDATE == '1' }}
        shell: bash
        run: |
          python -m pip install --disable-pip-version-check --no-input --no-cache-dir "jsonschema==4.23.0"
          python - <<'PY'
          import json, sys, pathlib
          from jsonschema import validate, Draft7Validator
          root = pathlib.Path(".")
          ndjson = root/".ci"/"local"/"telemetry.ndjson"
          schema_candidates = [
              root/".ci"/"schemas"/"telemetry.schema.json",
              root/".ci"/"telemetry.schema.json",
              root/"schemas"/"telemetry.schema.json",
              root/"telemetry.schema.json",
          ]
          schema = next((p for p in schema_candidates if p.exists()), None)
          if not schema or not ndjson.exists():
              print("telemetry: schema or ndjson not found; skipping", file=sys.stderr); sys.exit(0)
          sch = json.load(open(schema, "r", encoding="utf-8"))
          Draft7Validator.check_schema(sch)
          n = ok = 0
          for line in ndjson.read_text(encoding="utf-8").splitlines():
              line = line.strip()
              if not line:
                  continue
              validate(instance=json.loads(line), schema=sch)
              ok += 1
              n  += 1
          print(f"telemetry: validated {ok}/{n} records")
          PY

      - name: Validate artifacts manifest schema
        if: ${{ always() && env.TELEMETRY_VALIDATE == '1' }}
        shell: bash
        run: |
          python -m pip install --disable-pip-version-check --no-input --no-cache-dir "jsonschema==4.23.0"
          python - <<'PY'
          import json, sys, pathlib
          from jsonschema import validate, Draft7Validator
          root = pathlib.Path(".")
          manifest = root/"artifacts.manifest.json"
          schema_candidates = [
              root/".ci"/"schemas"/"manifest.schema.json",
              root/".ci"/"manifest.schema.json",
              root/"schemas"/"manifest.schema.json",
              root/"manifest.schema.json",
          ]
          schema = next((p for p in schema_candidates if p.exists()), None)
          if not schema or not manifest.exists():
              print("manifest: schema or artifacts.manifest.json not found; skipping", file=sys.stderr); sys.exit(0)
          sch = json.load(open(schema, "r", encoding="utf-8"))
          data = json.load(open(manifest, "r", encoding="utf-8"))
          Draft7Validator.check_schema(sch)
          validate(instance=data, schema=sch)
          print("manifest: OK")
          PY
      - name: Generate lock telemetry
        if: always()
        shell: bash
        run: |
          python scripts/generate_manifest.py \
            --generated-by "lock-verification" \
            --item "requirements.lock" "lock" \
            --item "requirements-dev.lock" "lock" \
            --output lock-telemetry.manifest.json
      - name: Build drift manifest
        if: failure()
        shell: bash
        run: |
          python scripts/generate_manifest.py \
            --generated-by "lock-verification" \
            --item "new.requirements.lock" "lock" \
            --item "new.requirements-dev.lock" "lock" \
            --item "lock-drift.contents.json" "meta" \
            --item "lock-drift.url.txt" "meta" \
            --output artifacts.manifest.json
      - name: Validate drift manifest schema
        if: failure()
        shell: bash
        run: |
          python -m pip install --disable-pip-version-check --no-input --no-cache-dir "jsonschema==4.23.0"
          python - <<'PY'
          import json
          from jsonschema import validate, Draft7Validator
          with open('artifacts.manifest.json', 'r', encoding='utf-8') as f:
              data = json.load(f)
          with open('schemas/manifest.schema.json', 'r', encoding='utf-8') as f:
              schema = json.load(f)
          Draft7Validator.check_schema(schema)
          validate(instance=data, schema=schema)
          print('drift manifest: OK')
          PY
      - name: Recompile locks for artifact (on fail)
        if: failure()
        run: |
          python -m pip install -U pip "pip-tools==7.5.0"
          pip-compile --allow-unsafe --generate-hashes -o new.requirements.lock requirements.txt
          pip-compile --allow-unsafe --generate-hashes -o new.requirements-dev.lock requirements-dev.txt
      - name: Upload recompiled locks
        if: failure()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: lock-drift
          path: |
            new.requirements.lock
            new.requirements-dev.lock
            artifacts.manifest.json
            lock-telemetry.manifest.json
          retention-days: 30

  lint-and-type-check:
    name: Lint and Type Check
    needs: lock-verification
    # Explicit least-privilege permissions (B1)
    permissions:
      contents: read
      actions: read
      checks: write
    runs-on: ubuntu-24.04
    steps:
      # remove any malformed 'uses: sMiNT0S/AIBugBench/.' step; use the pinned composite action
      - name: Base Python + dev tools
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@d7b2e2cf30c00020de02ef40e6c70c445c84e1b4
        with:
          python-version: ${{env.PYTHON_DEFAULT_VERSION}}
          install-dev: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809  # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-${{hashFiles('requirements.txt')}}
          restore-keys: |
            ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-
      - name: Lint with ruff
        run: ruff check .
      # Single mypy pass with Python 3.13 (current standard)
      # Explicit targets (no src/ layout): benchmark/, validation/, and run_benchmark.py.
      - name: "mypy (Python 3.13)"
        run: |
          python -m mypy ./benchmark ./validation ./run_benchmark.py --no-error-summary
      - name: Format check with ruff
        run: ruff format --check .
      - name: CLI smoke test
        run: |
          python - <<'PY'
          import runpy
          ns = runpy.run_path('run_benchmark.py')
          assert callable(ns.get('main')), 'main() missing'
          print('CLI smoke test: main() found')
          PY

  test-coverage:
    name: Test Coverage Analysis
    needs: lint-and-type-check
    permissions:
      contents: read
      actions: read
      checks: write
    runs-on: ubuntu-24.04
    steps:
      - name: Base Python + test tooling
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@d7b2e2cf30c00020de02ef40e6c70c445c84e1b4
        with:
          python-version: ${{env.PYTHON_DEFAULT_VERSION}}
          install-test: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809  # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-${{hashFiles('requirements.txt')}}
          restore-keys: |
            ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-

      - name: Run tests (branch coverage unified)
        run: |
          coverage erase
          pytest -q \
            --cov=benchmark \
            --cov=validation \
            --cov=run_benchmark \
            --cov-branch \
            --cov-report=term-missing \
            --cov-report=xml:coverage.xml \
            --cov-report=html:htmlcov
          coverage combine || true
            # Export XML for Codecov
            coverage xml -o coverage.xml || true
          coverage report -m

      - name: Coverage summary
        run: |
          echo "## Test Coverage (Branch Mode)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          coverage report --format=markdown >> $GITHUB_STEP_SUMMARY

      - name: Detect coverage.xml presence
        id: coverage_file
        shell: bash
        run: |
          if [ -f coverage.xml ]; then
            echo "present=true" >> "$GITHUB_OUTPUT"
            echo "coverage.xml present"
          else
            echo "present=false" >> "$GITHUB_OUTPUT"
            echo "coverage.xml missing"
          fi

      # New: publish the HTML coverage report for PR browsing
      - name: Upload HTML coverage
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: htmlcov
          path: htmlcov
          if-no-files-found: warn
          retention-days: 7
      # Detect presence of Codecov secret without job-level env
      - name: Upload coverage to Codecov
        if: ${{ hashFiles('coverage.xml') != '' && runner.os == 'Linux' }}
        uses: codecov/codecov-action@5a1091511ad55cbe89839c7260b706298ca349f7
        with:
          files: coverage.xml
          flags: unittests
          fail_ci_if_error: true
          verbose: true
  multi-platform-tests:
    name: Multi-Platform Tests
    needs: test-coverage
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-24.04, windows-2022, macos-15]
        python-version: ["3.13.7"]
        include:
          - os: ubuntu-24.04
            platform_name: linux
          - os: windows-2022
            platform_name: windows
          - os: macos-15
            platform_name: macos
    runs-on: ${{matrix.os}}
    env:
      PEP517_BUILD_DEBUG: "1"
    permissions:
      contents: read
      actions: read
      checks: write
    steps:
      - name: Debug | Pre-build egg-info snapshot (Linux/macOS)
        if: runner.os != 'Windows'
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p .ci
          echo "[pre-clean] Listing *.egg-info under repo root (recursive)" | tee .ci/egginfo-pre-clean.txt
          find . -name '*.egg-info' -print | tee -a .ci/egginfo-pre-clean.txt || true
      - name: Debug | Pre-build egg-info snapshot (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          New-Item -Force -ItemType Directory .ci | Out-Null
          "[pre-clean] Listing *.egg-info under repo root (recursive)" | Out-File -Encoding UTF8 .ci/egginfo-pre-clean.txt
          Get-ChildItem -Path . -Recurse -Filter '*.egg-info' -ErrorAction SilentlyContinue | ForEach-Object { $_.FullName } | Out-File -Append -Encoding UTF8 .ci/egginfo-pre-clean.txt

      - name: Base Python + test tooling
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@d7b2e2cf30c00020de02ef40e6c70c445c84e1b4
        with:
          python-version: ${{matrix.python-version}}
          install-test: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809  # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{runner.os}}-pip-${{matrix.python-version}}-${{hashFiles('requirements.txt')}}
          restore-keys: |
            ${{runner.os}}-pip-${{matrix.python-version}}-

      - name: Cleanup | Remove stale build artifacts
        shell: bash
        run: |
          echo "Cleaning stale build artifacts (.egg-info, build/, dist/)"
          find . -type d -name '*.egg-info' -prune -print -exec rm -rf {} + || true
          find . -type d -name 'build'      -prune -print -exec rm -rf {} + || true
          find . -type d -name 'dist'       -prune -print -exec rm -rf {} + || true

      - name: Debug | Post-clean egg-info snapshot (Linux/macOS)
        if: runner.os != 'Windows'
        shell: bash
        run: |
          set -euo pipefail
          echo "[post-clean] Listing *.egg-info under repo root (recursive)" | tee .ci/egginfo-post-clean.txt
          find . -name '*.egg-info' -print | tee -a .ci/egginfo-post-clean.txt || true
      - name: Debug | Post-clean egg-info snapshot (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          "[post-clean] Listing *.egg-info under repo root (recursive)" | Out-File -Encoding UTF8 .ci/egginfo-post-clean.txt
          Get-ChildItem -Path . -Recurse -Filter '*.egg-info' -ErrorAction SilentlyContinue | ForEach-Object { $_.FullName } | Out-File -Append -Encoding UTF8 .ci/egginfo-post-clean.txt

      - name: Debug | Tool versions
        run: |
          python -V
          python -c "import setuptools, wheel; print('setuptools', setuptools.__version__); print('wheel', wheel.__version__)"
          python -m pip --version

      - name: Install package and capture logs
        run: |
          python -m pip install -vvv --use-pep517 --progress-bar off --log .ci/pip-debug.log .
        continue-on-error: true

      - name: Upload debug artifacts on failure
        if: failure()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: pep517-debug-${{ matrix.platform_name }}-py${{ matrix.python-version }}
          path: |
            .ci/egginfo-pre-clean.txt
            .ci/egginfo-post-clean.txt
            .ci/pip-debug.log
            # The wrapper script will create these files in the project root
            _all_candidates.txt

      - name: Final package installation attempt (fail job if needed)
        # This step exists to ensure the job actually fails if the install fails.
        # The previous step continues on error to allow artifact upload.
        run: python -m pip install .

      # ... (the rest of your test steps can follow) ...
      - name: Run comprehensive test suite
        run: |
          echo "Running comprehensive test suite on ${{matrix.os}} with Python ${{matrix.python-version}}"
          pytest tests/ -v --tb=short -m "not slow"

  benchmark-consistency-check:
    name: Benchmark Consistency Validation
    needs: multi-platform-tests
    runs-on: ubuntu-24.04
    if: always()
    permissions:
      contents: read
      actions: read
      checks: write
    steps:
      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
      - name: Base Python setup
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@d7b2e2cf30c00020de02ef40e6c70c445c84e1b4
        with:
          python-version: ${{env.PYTHON_DEFAULT_VERSION}}

      - name: Download all benchmark artifacts
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0
        with:
          pattern: benchmark-results-*
          path: ./collected-results
          merge-multiple: true

      - name: Check collected-results availability
        id: check_collected
        shell: bash
        run: |
          if [ ! -d ./collected-results ]; then
            echo "No collected-results directory found"
            echo "has_artifacts=false" >> "$GITHUB_OUTPUT"
          elif [ -z "$(ls -A ./collected-results 2>/dev/null)" ]; then
            echo "collected-results directory is empty"
            echo "has_artifacts=false" >> "$GITHUB_OUTPUT"
          else
            echo "Found benchmark artifacts in collected-results"
            echo "has_artifacts=true" >> "$GITHUB_OUTPUT"
            ls -la ./collected-results
          fi

      - name: Skip benchmark comparison (no artifacts)
        if: steps.check_collected.outputs.has_artifacts != 'true'
        run: |
          echo "WARNING: No benchmark artifacts found. Skipping consistency validation."
          echo "This may happen if platform validation failed on all platforms."
          echo "Check the multi-platform-tests job logs for errors."
          echo "# Benchmark Consistency Report" > consistency_report.md
          echo "" >> consistency_report.md
          echo "**Status:** No artifacts available for comparison" >> consistency_report.md
          echo "**Reason:** Platform validation may have failed on all test platforms" >> consistency_report.md

      - name: Run cross-platform comparison
        id: comparison
        if: steps.check_collected.outputs.has_artifacts == 'true'
        shell: bash
        run: python scripts/compare_benchmarks.py --dir ./collected-results
        continue-on-error: false

      - name: Generate consistency report
        if: steps.check_collected.outputs.has_artifacts == 'true'
        run: |
          echo "# Benchmark Consistency Report" > consistency_report.md
          echo "" >> consistency_report.md
          echo "**Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> consistency_report.md
          echo "**Commit:** ${{github.sha}}" >> consistency_report.md
          echo "" >> consistency_report.md

          if ls ./collected-results/platform_validation_*.json 1> /dev/null 2>&1; then
            echo "## Platform Results" >> consistency_report.md
            for file in ./collected-results/platform_validation_*.json; do
              echo "### $(basename "$file")" >> consistency_report.md
              python -c "
              import json
              with open('$file', 'r') as f:
                  data = json.load(f)
              print(f'- **Platform:** {data.get(\"platform\", \"unknown\")}')
              print(f'- **Score:** {data.get(\"total_score\", 0):.2f}/100')
              print(f'- **Within Tolerance:** {data.get(\"within_tolerance\", false)}')
              if 'full_results' in data and 'execution_time' in data['full_results']:
                  print(f'- **Execution Time:** {data[\"full_results\"][\"execution_time\"]:.2f}s')
              " >> consistency_report.md
              echo "" >> consistency_report.md
            done
          else
            echo "No platform validation results found" >> consistency_report.md
          fi

      - name: Upload consistency report
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: consistency-report
          path: |
            consistency_report.md
            collected-results/
          retention-days: 30

  security-scan:
    name: Security Scan
    needs: lock-verification
    runs-on: ubuntu-24.04
    permissions:
      contents: read
      actions: read
      checks: write
    steps:
      - name: Base Python + security tooling
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@d7b2e2cf30c00020de02ef40e6c70c445c84e1b4
        with:
          python-version: ${{env.PYTHON_DEFAULT_VERSION}}
          install-security: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809  # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-${{hashFiles('requirements.txt')}}
          restore-keys: |
            ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-

      - name: Run bandit security scan
        # Use pyproject.toml configuration for consistent exclusions (pinned version)
        run: |
          bandit \
            -r . \
            -ll \
            --configfile pyproject.toml \
            -f json \
            -o bandit-report.json \
            --version
        continue-on-error: true

      - name: Run safety scan
        run: |
          safety scan \
            --file=requirements.lock \
            --json \
            > safety-report.json \
            || echo "safety scan fallback"
        continue-on-error: true
      - name: Generate security telemetry
        if: always()
        shell: bash
        run: |
          python scripts/generate_manifest.py \
            --generated-by "security-scan" \
            --item "bandit-report.json" "security" \
            --item "safety-report.json" "security" \
            --item "audit.json" "security" \
            --output security-telemetry.manifest.json
      - name: Run internal security audit
        run: python scripts/security_audit.py --json --output audit.json
        continue-on-error: true

      - name: Fail if audit not ok
        run: |
          # Inline Python audit verification wrapped for readability (was a single long line)
          python - <<'PY'
          import json, sys, pathlib
          data = json.loads(pathlib.Path('audit.json').read_text())
          ok = bool(data.get('ok'))
          print('Audit OK:', ok)
          sys.exit(0 if ok else 1)
          PY
        continue-on-error: false

      - name: Generate security audit summary
        if: always()
        run: python scripts/generate_audit_summary.py audit.json || true

      - name: Upload security reports
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
            audit.json
            audit_summary.md
            security-telemetry.manifest.json
          retention-days: 30
