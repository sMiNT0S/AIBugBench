name: CI

env:
  PYTHON_DEFAULT_VERSION: "3.13"
  BANDIT_VERSION: "1.8.6"
  RUFF_VERSION: "0.12.11"  # Upgraded to align with local environment (supports UP047 rule)
  MYPY_VERSION: "1.11.2"
  PYTEST_VERSION: "8.3.2"
  PYTEST_COV_VERSION: "5.0.0"
  COVERAGE_VERSION: "7.6.1"
  SAFETY_VERSION: "3.2.5"

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

concurrency:
  group: ${{github.workflow}}-${{github.ref}}
  cancel-in-progress: true

# (Moved to top-level env block)

jobs:
  action-pin-check:
    name: Actions Pin Verification
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
      - name: Verify all actions pinned to full SHAs
        run: |
          if grep -R "uses: .*@v[0-9]" .github/workflows/*.yml; then
            echo "Found unpinned actions. Pin to full SHAs." >&2
            exit 1
          fi

  lock-verification:
    name: Dependency Lock Sync (runtime + dev)
    needs: action-pin-check
    permissions:
      contents: read
      actions: read
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
      - name: Set up Python ${{ env.PYTHON_DEFAULT_VERSION }}
        uses: actions/setup-python@e797f83bcb11b83ae66e0230d6156d7c80228e7c

        with:
          python-version: ${{env.PYTHON_DEFAULT_VERSION}}
      - name: Debug lock environment
        shell: bash
        run: |
          set -eu
          python --version
          python -c "import importlib.metadata as m; print('pip-tools', m.version('pip-tools'))" || true
          git config --global core.autocrlf false || true
      - name: Verify dependency lock determinism
        env:
          PYTHONUTF8: "1"
        run: |
          python -m pip install -U pip
          python -m pip install "pip-tools==7.5.0"
          # Uses our canonicalizer: LF-only write + sorted/deduped '# via ...'
          python scripts/update_requirements_lock.py --check || (echo "runtime lock drift"; exit 3)
          python scripts/update_requirements_lock.py --check --dev || (echo "dev lock drift"; exit 3)
      - name: Verify dependency lock determinism (JSON)
        if: always()
        shell: bash
        run: |
          python -m pip install "pip-tools==7.5.0"
          python scripts/update_requirements_lock.py --check --json || true
          python scripts/update_requirements_lock.py --check --dev --json || true
      - name: Build drift manifest
        if: failure()
        shell: bash
        run: |
          cat > artifacts.manifest.json <<'JSON'
          {"items":[
            {"path":"new.requirements.lock","kind":"lock"},
            {"path":"new.requirements-dev.lock","kind":"lock"}
          ]}
          JSON
      - name: Recompile locks for artifact (on fail)
        if: failure()
        run: |
          python -m pip install -U pip "pip-tools==7.5.0"
          pip-compile --allow-unsafe --generate-hashes -o new.requirements.lock requirements.txt
          pip-compile --allow-unsafe --generate-hashes -o new.requirements-dev.lock requirements-dev.txt
      - name: Upload recompiled locks
        if: failure()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: lock-drift
          path: |
            new.requirements.lock
            new.requirements-dev.lock
            artifacts.manifest.json
          retention-days: 30

  lint-and-type-check:
    name: Lint and Type Check
    needs: lock-verification
    # Explicit least-privilege permissions (B1)
    permissions:
      contents: read
      actions: read
    runs-on: ubuntu-latest
    steps:
      # remove any malformed 'uses: sMiNT0S/AIBugBench/.' step; use the pinned composite action
      - name: Base Python + dev tools
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@ab149f501046408f307d3ac6a7c0991e08b74d61
        with:
          python-version: ${{env.PYTHON_DEFAULT_VERSION}}
          install-dev: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809  # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-${{hashFiles('requirements.txt')}}
          restore-keys: |
            ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-
      - name: Lint with ruff
        run: ruff check .
      # Single mypy pass with Python 3.13 (current standard)
      # Explicit targets (no src/ layout): benchmark/, validation/, and run_benchmark.py.
      - name: "mypy (Python 3.13)"
        run: |
          python -m mypy ./benchmark ./validation ./run_benchmark.py --no-error-summary
      - name: Format check with ruff
        run: ruff format --check .
      - name: CLI smoke test
        run: python -c "import run_benchmark as m; assert callable(getattr(m,'main',None))"

  test-coverage:
    name: Test Coverage Analysis
    needs: lint-and-type-check
    permissions:
      contents: read
      actions: read
    env:
      CODECOV_TOKEN: ${{secrets.CODECOV_TOKEN}}
    runs-on: ubuntu-latest
    steps:
      - name: Base Python + test tooling
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@ab149f501046408f307d3ac6a7c0991e08b74d61
        with:
          python-version: ${{env.PYTHON_DEFAULT_VERSION}}
          install-test: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809  # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-${{hashFiles('requirements.txt')}}
          restore-keys: |
            ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-

      - name: Run tests (branch coverage unified)
        run: |
          coverage erase
          pytest -q \
            --cov=benchmark \
            --cov=validation \
            --cov=run_benchmark \
            --cov-branch \
            --cov-report=term-missing \
            --cov-report=xml:coverage.xml \
            --cov-report=html:htmlcov
          coverage combine || true
          coverage report -m

      - name: Coverage summary
        run: |
          echo "## Test Coverage (Branch Mode)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          coverage report --format=markdown >> $GITHUB_STEP_SUMMARY

      - name: Detect coverage.xml presence
        id: coverage_file
        shell: bash
        run: |
          if [ -f coverage.xml ]; then
            echo "present=true" >> "$GITHUB_OUTPUT"
            echo "coverage.xml present"
          else
            echo "present=false" >> "$GITHUB_OUTPUT"
            echo "coverage.xml missing"
          fi

      # New: publish the HTML coverage report for PR browsing
      - name: Upload HTML coverage
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: htmlcov
          path: htmlcov
          if-no-files-found: warn
          retention-days: 7
      # Detect presence of Codecov secret without job-level env
      - name: Detect Codecov token
        shell: bash
        run: |
          if [ -n "${{secrets.CODECOV_TOKEN}}" ]; then
          echo "HAVE_CODECOV=1" >> "$GITHUB_ENV"
          else
          echo "HAVE_CODECOV=0" >> "$GITHUB_ENV"
          fi

      - name: Upload to Codecov
        if: ${{ env.HAVE_CODECOV == '1' && hashFiles('coverage.xml') != '' }}
        uses: codecov/codecov-action@5a1091511ad55cbe89839c7260b706298ca349f7

        with:
          files: coverage.xml
          token: ${{secrets.CODECOV_TOKEN}}
          fail_ci_if_error: true

      - name: Skip Codecov (no token present)
        if: ${{env.HAVE_CODECOV != '1' || steps.coverage_file.outputs.present != 'true'}}
        run: |
          if [ "$HAVE_CODECOV" != '1' ]; then
            echo "Codecov token not provided; skipping upload (private repo)."
          elif [ "${{steps.coverage_file.outputs.present}}" != 'true' ]; then
            echo "coverage.xml missing; skipping Codecov upload."
          fi
  multi-platform-tests:
    name: Multi-Platform Tests
    needs: test-coverage
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.13"]
        include:
          - os: ubuntu-latest
            platform_name: linux
          - os: windows-latest
            platform_name: windows
          - os: macos-latest
            platform_name: macos
    runs-on: ${{matrix.os}}
    permissions:
      contents: read
      actions: read
    steps:
      - name: Base Python + test tooling
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@ab149f501046408f307d3ac6a7c0991e08b74d61
        with:
          python-version: ${{matrix.python-version}}
          install-test: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809  # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{runner.os}}-pip-${{matrix.python-version}}-${{hashFiles('requirements.txt')}}
          restore-keys: |
            ${{runner.os}}-pip-${{matrix.python-version}}-
      - name: Install package (editable)
        run: python -m pip install -e .

      - name: Run comprehensive test suite
        run: |
          echo "Running comprehensive test suite on ${{matrix.os}} with Python ${{matrix.python-version}}"
          pytest tests/ -v --tb=short -m "not slow"

      - name: Run unit tests (fallback)
        if: failure()
        run: |
          echo "Full suite failed; executing minimal import smoke test for diagnostics..."
          pytest tests/test_imports.py -v --tb=short || true
          echo "Primary test suite failure retained (job will fail)."

      - name: Validate test data structure
        shell: bash
        run: |
          echo "Validating test data files exist..."
          test -f test_data/process_records.py
          test -f test_data/user_data.json
          test -f test_data/config.yaml
          echo "Test data validation passed"

      - name: Run platform-specific benchmark validation
        run: python benchmark/platform_validator.py
        continue-on-error: true

      - name: Check for benchmark artifacts
        id: check_artifacts
        shell: bash
        run: |
          if [ -d "ci_results" ] && [ -n "$(ls -A ci_results 2>/dev/null)" ]; then
            echo "has_ci_results=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_ci_results=false" >> "$GITHUB_OUTPUT"
          fi
          if [ -f "*.log" ] 2>/dev/null; then
            echo "has_logs=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_logs=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Collect benchmark artifacts
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always() && (steps.check_artifacts.outputs.has_ci_results == 'true' || steps.check_artifacts.outputs.has_logs == 'true')
        with:
          name: benchmark-results-${{matrix.platform_name}}-py${{matrix.python-version}}
          path: |
            ci_results/
            results/
            *.log
          retention-days: 30
          if-no-files-found: warn

  benchmark-consistency-check:
    name: Benchmark Consistency Validation
    needs: multi-platform-tests
    runs-on: ubuntu-latest
    if: always()
    permissions:
      contents: read
      actions: read
    steps:
      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
      - name: Base Python setup
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@ab149f501046408f307d3ac6a7c0991e08b74d61
        with:
          python-version: ${{env.PYTHON_DEFAULT_VERSION}}

      - name: Download all benchmark artifacts
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0
        with:
          pattern: benchmark-results-*
          path: ./collected-results
          merge-multiple: true

      - name: Check collected-results availability
        id: check_collected
        shell: bash
        run: |
          if [ ! -d ./collected-results ]; then
            echo "No collected-results directory found"
            echo "has_artifacts=false" >> "$GITHUB_OUTPUT"
          elif [ -z "$(ls -A ./collected-results 2>/dev/null)" ]; then
            echo "collected-results directory is empty"
            echo "has_artifacts=false" >> "$GITHUB_OUTPUT"
          else
            echo "Found benchmark artifacts in collected-results"
            echo "has_artifacts=true" >> "$GITHUB_OUTPUT"
            ls -la ./collected-results
          fi

      - name: Skip benchmark comparison (no artifacts)
        if: steps.check_collected.outputs.has_artifacts != 'true'
        run: |
          echo "WARNING: No benchmark artifacts found. Skipping consistency validation."
          echo "This may happen if platform validation failed on all platforms."
          echo "Check the multi-platform-tests job logs for errors."
          echo "# Benchmark Consistency Report" > consistency_report.md
          echo "" >> consistency_report.md
          echo "**Status:** No artifacts available for comparison" >> consistency_report.md
          echo "**Reason:** Platform validation may have failed on all test platforms" >> consistency_report.md

      - name: Run cross-platform comparison
        id: comparison
        if: steps.check_collected.outputs.has_artifacts == 'true'
        shell: bash
        run: python scripts/compare_benchmarks.py --dir ./collected-results
        continue-on-error: false

      - name: Generate consistency report
        if: steps.check_collected.outputs.has_artifacts == 'true'
        run: |
          echo "# Benchmark Consistency Report" > consistency_report.md
          echo "" >> consistency_report.md
          echo "**Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> consistency_report.md
          echo "**Commit:** ${{github.sha}}" >> consistency_report.md
          echo "" >> consistency_report.md

          if ls ./collected-results/platform_validation_*.json 1> /dev/null 2>&1; then
            echo "## Platform Results" >> consistency_report.md
            for file in ./collected-results/platform_validation_*.json; do
              echo "### $(basename "$file")" >> consistency_report.md
              python -c "
              import json
              with open('$file', 'r') as f:
                  data = json.load(f)
              print(f'- **Platform:** {data.get(\"platform\", \"unknown\")}')
              print(f'- **Score:** {data.get(\"total_score\", 0):.2f}/100')
              print(f'- **Within Tolerance:** {data.get(\"within_tolerance\", false)}')
              if 'full_results' in data and 'execution_time' in data['full_results']:
                  print(f'- **Execution Time:** {data[\"full_results\"][\"execution_time\"]:.2f}s')
              " >> consistency_report.md
              echo "" >> consistency_report.md
            done
          else
            echo "No platform validation results found" >> consistency_report.md
          fi

      - name: Upload consistency report
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: consistency-report
          path: |
            consistency_report.md
            collected-results/
          retention-days: 90

  security-scan:
    name: Security Scan
    needs: lock-verification
    runs-on: ubuntu-latest
    permissions:
      contents: read
      actions: read
    steps:
      - name: Base Python + security tooling
        uses: sMiNT0S/AIBugBench/.github/actions/base-python-setup@ab149f501046408f307d3ac6a7c0991e08b74d61
        with:
          python-version: ${{env.PYTHON_DEFAULT_VERSION}}
          install-security: "true"
      - name: Cache pip
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809  # v4.2.4
        with:
          path: ~/.cache/pip
          key: ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-${{hashFiles('requirements.txt')}}
          restore-keys: |
            ${{runner.os}}-pip-${{env.PYTHON_DEFAULT_VERSION}}-

      - name: Run bandit security scan
        # Use pyproject.toml configuration for consistent exclusions (pinned version)
        run: |
          bandit \
            -r . \
            -ll \
            --configfile pyproject.toml \
            -f json \
            -o bandit-report.json \
            --version
        continue-on-error: true

      - name: Run safety scan
        run: |
          safety scan \
            --file=requirements.lock \
            --json \
            > safety-report.json \
            || echo "safety scan fallback"
        continue-on-error: true
      - name: Run internal security audit
        run: python scripts/security_audit.py --json --output audit.json
        continue-on-error: true

      - name: Fail if audit not ok
        run: |
          # Inline Python audit verification wrapped for readability (was a single long line)
          python - <<'PY'
          import json, sys, pathlib
          data = json.loads(pathlib.Path('audit.json').read_text())
          ok = bool(data.get('ok'))
          print('Audit OK:', ok)
          sys.exit(0 if ok else 1)
          PY
        continue-on-error: false

      - name: Generate security audit summary
        if: always()
        run: python scripts/generate_audit_summary.py audit.json || true

      - name: Upload security reports
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
            audit.json
            audit_summary.md
          retention-days: 30
