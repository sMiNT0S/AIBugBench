# API Reference

Complete reference for AIBugBench CLI commands, configuration options, and Python API.

## Command Line Interface

### Primary Usage

```bash
python run_benchmark.py --model your_model_name
```

### Command Line Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--model` | string | None | Model name to test (required unless testing all) |
| `--results-dir` | path | `results/` | Custom results output directory |
| `--timeout` | int | 30 | Per-operation timeout in seconds |
| `--verbose` | flag | False | Enable verbose diagnostic output |
| `--debug` | flag | False | Extended internal logging (implementation details) |
| `--quiet` | flag | False | Suppress non-essential output |
| `--all-models` | flag | False | Test all models in submissions directory |

### Examples

**Test a specific model:**

```bash
python run_benchmark.py --model gpt4_turbo
```

**Test with custom results directory:**

```bash
python run_benchmark.py --model claude_opus --results-dir custom_results/
```

**Test with verbose output and extended timeout:**

```bash
python run_benchmark.py --model llama_70b --verbose --timeout 60
```

**Test all models:**

```bash
python run_benchmark.py --all-models
```

**Quiet mode for CI/CD:**

```bash
python run_benchmark.py --model example_model --quiet
```

## Configuration Files

### Test Data Configuration

Configuration files are generated by `scripts/bootstrap_repo.py` and located in `test_data/`:

- `config.yaml` - Deliberately broken YAML configuration (multi-document)
- `user_data.json` - Sample user data for transformation
- `process_records.py` - Python script requiring refactoring

### Loading Configuration

**Safe YAML Loading:**

```python
import yaml

# Load multi-document YAML safely
with open('test_data/config.yaml', 'r') as f:
    docs = list(yaml.safe_load_all(f))
    # Merge documents (last document wins)
    config = {}
    for doc in docs:
        if doc:
            config.update(doc)
```

**Configuration Structure:**

```yaml
use_legacy_paths: true
paths:
  data_source: /srv/data/production/users.json
  legacy_data_source: ./user_data.json
  log_file: /var/log/processor.log
validation_rules:
  min_age: 18
  max_age: 120
  required_fields:
    - name
    - email
    - country
processing_options:
  batch_size: 100
  timeout_seconds: 30
  retry_attempts: 3
api_keys:
  - primary_key
  - secondary_key
  - backup_key
feature_flags:
  enable_logging: true
  strict_validation: false
  debug_mode: false
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `AIBUGBENCH_RESULTS_DIR` | `results/` | Override default results directory |
| `AIBUGBENCH_TIMEOUT` | `30` | Default operation timeout |
| `AIBUGBENCH_DEBUG` | `false` | Enable debug logging |
| `PYTHONPATH` | - | Include benchmark modules in path |

## Python API

### Core Modules

#### benchmark.runner

Main benchmark execution module.

```python
from benchmark.runner import BenchmarkRunner

# Initialize runner
runner = BenchmarkRunner(
    model_name="gpt4",
    results_dir="results/",
    timeout=30
)

# Run all tests
results = runner.run_all_tests()

# Run specific test
prompt1_result = runner.run_test("prompt_1_refactoring")
```

#### benchmark.validators

Validation logic for each prompt.

```python
from benchmark.validators import (
    validate_prompt1_refactoring,
    validate_prompt2_yaml_json,
    validate_prompt3_transformation,
    validate_prompt4_api_integration
)

# Validate refactored code
result = validate_prompt1_refactoring(
    solution_path="submissions/model/prompt_1_solution.py"
)
print(f"Score: {result['score']}/{result['max_score']}")
```

#### benchmark.scoring

Scoring engine with 7-category assessment.

```python
from benchmark.scoring import calculate_score, get_grade

# Calculate total score
results = {
    'prompt_1': {'score': 23.5, 'max_score': 25},
    'prompt_2': {'score': 25.0, 'max_score': 25},
    'prompt_3': {'score': 24.5, 'max_score': 25},
    'prompt_4': {'score': 21.0, 'max_score': 25}
}

total_score = calculate_score(results)
grade = get_grade(total_score)
print(f"Total: {total_score}/100 - Grade: {grade}")
```

#### benchmark.utils

Utility functions for file operations and formatting.

```python
from benchmark.utils import (
    safe_load_json,
    safe_load_yaml,
    format_results,
    create_comparison_chart
)

# Safe file loading with error handling
data = safe_load_json("user_data.json")
config = safe_load_yaml("config.yaml")

# Format results for display
formatted = format_results(results)
print(formatted)

# Create comparison chart
chart = create_comparison_chart(results)
print(chart)
```

### Validation Functions

#### Prompt 1: Code Refactoring

```python
def validate_prompt1_refactoring(solution_path: str) -> dict:
    """
    Validate refactored Python code.
    
    Returns:
        dict: {
            'score': float,
            'max_score': 25,
            'passed': bool,
            'details': {
                'syntax': bool,
                'execution': bool,
                'security': list,
                'performance': list,
                'maintainability': list
            }
        }
    """
```

#### Prompt 2: YAML/JSON Correction

```python
def validate_prompt2_yaml_json(yaml_path: str, json_path: str) -> dict:
    """
    Validate corrected YAML and JSON files.
    
    Returns:
        dict: {
            'score': float,
            'max_score': 25,
            'passed': bool,
            'details': {
                'yaml_valid': bool,
                'json_valid': bool,
                'equivalence': bool,
                'structure': dict
            }
        }
    """
```

#### Prompt 3: Data Transformation

```python
def validate_prompt3_transformation(transform_path: str) -> dict:
    """
    Validate data transformation function.
    
    Returns:
        dict: {
            'score': float,
            'max_score': 25,
            'passed': bool,
            'details': {
                'function_exists': bool,
                'signature_correct': bool,
                'transformations': dict,
                'business_rules': bool
            }
        }
    """
```

#### Prompt 4: API Integration

```python
def validate_prompt4_api_integration(api_path: str) -> dict:
    """
    Validate API integration function.
    
    Returns:
        dict: {
            'score': float,
            'max_score': 25,
            'passed': bool,
            'details': {
                'function_exists': bool,
                'authentication': bool,
                'error_handling': dict,
                'security': dict
            }
        }
    """
```

## Output Formats

### JSON Results Format

```json
{
  "model": "gpt4_turbo",
  "timestamp": "2025-01-15T10:30:00",
  "total_score": 94.25,
  "grade": "A",
  "prompts": {
    "prompt_1": {
      "score": 23.5,
      "max_score": 25,
      "passed": true,
      "categories": {
        "syntax": 5.0,
        "structure": 3.0,
        "execution": 6.0,
        "quality": 3.0,
        "security": 4.0,
        "performance": 1.5,
        "maintainability": 1.0
      }
    }
  }
}
```

### Summary Report Format

================================================================================
BENCHMARK RESULTS - gpt4_turbo
================================================================================

Prompt 1: Code Refactoring & Analysis
  Score: 23.50/25 (94.0%)
  ✅ Syntax Validation (5.0/5.0)
  ✅ Code Structure (3.0/3.0)
  ✅ Execution Success (6.0/6.0)
  ✅ Code Quality (3.0/3.0)
  ✅ Security Analysis (4.0/4.0)
  ⚠️ Performance Analysis (1.5/2.0)
  ⚠️ Maintainability Analysis (1.0/2.0)

[Additional prompts...]

FINAL SCORE: 94.25/100 (94.3%) - Grade: A

## Comparison Chart Format

Model Comparison:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

gpt4_turbo      [████████████████████████████████████████████░░░░] 94.3%
claude_opus     [███████████████████████████████████████████░░░░░] 91.8%
llama_70b       [████████████████████████████████████░░░░░░░░░░░░] 76.5%
example_model   [██████████████████████████████████████████████░░] 92.2%

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

## Error Handling

### Common Exit Codes

| Code | Meaning | Resolution |
|------|---------|------------|
| 0 | Success | - |
| 1 | General error | Check error message |
| 2 | Missing model | Verify model name and directory |
| 3 | Validation failure | Review submission files |
| 4 | Timeout exceeded | Increase timeout or optimize code |
| 5 | File not found | Ensure all required files exist |

### Error Messages

**Model not found:**

Error: Model 'invalid_model' not found in submissions directory

**Missing file:**

Error: Required file 'prompt_1_solution.py' not found for model 'gpt4'

**Syntax error:**

Error: Python syntax error in prompt_1_solution.py:
  File "prompt_1_solution.py", line 10
    print("Missing closing quote)
                                 ^
SyntaxError: unterminated string literal

## Platform-Specific Notes

### Windows

- Use `python` or `py` command
- Paths use backslashes or forward slashes
- PowerShell may require execution policy adjustment

### macOS/Linux

- Use `python3` command
- Ensure proper file permissions
- May need to use `sudo` for certain operations

### Docker

- Mount submissions directory as volume
- Set environment variables in container
- Use non-root user for security

## See Also

- **[Getting Started](getting-started.md)** - Initial setup and quick start
- **[Developer Guide](developer-guide.md)** - Adding and testing models
- **[Scoring Methodology](scoring-methodology.md)** - Understanding scores
- **[Troubleshooting](troubleshooting.md)** - Common issues and solutions
